{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","machine_shape":"hm"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":16,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bpvFSwqrFAr_","outputId":"34da30f0-db50-41d6-9a69-c3656b3a5f6c","executionInfo":{"status":"ok","timestamp":1702124589278,"user_tz":300,"elapsed":24833,"user":{"displayName":"colabpro gatechmscs","userId":"12293304342394863693"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["The autoreload extension is already loaded. To reload it, use:\n","  %reload_ext autoreload\n","Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["# !rm -rf /content/sumanth_setup\n","\n","%load_ext autoreload\n","%autoreload 2\n","\n","import locale\n","def getpreferredencoding(do_setlocale = True):\n","    return \"UTF-8\"\n","locale.getpreferredencoding = getpreferredencoding\n","\n","!pip -q install datasets\n","!pip -q install transformers\n","!pip -q install peft\n","!pip -q install -U bitsandbytes\n","!pip -q install rouge\n","\n","\n","import numpy as np\n","from datasets import load_dataset\n","\n","from transformers import AutoTokenizer, AutoModelForCausalLM\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","#%cd /content/drive/MyDrive/CS6220 Folder #add direct access to the folder if you get an error in this cell\n","import os\n","os.chdir(\"/content/drive/MyDrive/CS6220 Folder\")"]},{"cell_type":"code","source":["device=\"cuda\"\n","!pip install huggingface-cli\n","\n","!huggingface-cli login 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"],"metadata":{"id":"nBg6RHOAFMey","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1702124748700,"user_tz":300,"elapsed":1352,"user":{"displayName":"colabpro gatechmscs","userId":"12293304342394863693"}},"outputId":"6c03d8f1-c133-4f05-8332-00c4cace9727"},"execution_count":17,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[31mERROR: Could not find a version that satisfies the requirement huggingface-cli (from versions: none)\u001b[0m\u001b[31m\n","\u001b[0m\u001b[31mERROR: No matching distribution found for huggingface-cli\u001b[0m\u001b[31m\n","\u001b[0musage: huggingface-cli <command> [<args>]\n","huggingface-cli: error: unrecognized arguments: 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\n"]}]},{"cell_type":"code","source":["import numpy as np\n","from datasets import load_dataset, concatenate_datasets\n","\n","from transformers import (\n","    AutoTokenizer,\n","    AutoModelWithLMHead,\n","    AutoModelForCausalLM,\n","    BitsAndBytesConfig,\n","    GenerationConfig,\n",")\n","from nltk.translate.bleu_score import sentence_bleu\n","import os\n","\n","\n","from datasets import load_dataset\n","\n","import torch\n","from torch.utils.data import Dataset\n","\n","from peft import LoraConfig, prepare_model_for_kbit_training, get_peft_model\n","\n","import pickle\n","\n","import time\n","from tqdm import tqdm\n","\n","import copy\n","\n","import random\n","\n","from bs4 import BeautifulSoup\n"],"metadata":{"id":"9VZ9Kk1LFPjX","executionInfo":{"status":"ok","timestamp":1702124750910,"user_tz":300,"elapsed":158,"user":{"displayName":"colabpro gatechmscs","userId":"12293304342394863693"}}},"execution_count":18,"outputs":[]},{"cell_type":"code","source":["!mkdir /content/sumanth_setup\n","!cp './helpers/' -r '/content/sumanth_setup'"],"metadata":{"id":"KuEvZmolSpzb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["os.chdir('/content/sumanth_setup')"],"metadata":{"id":"L4hdOPiPSiiD"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Load MBPP Dataset\n"],"metadata":{"id":"2xDx_OSXFgZf"}},{"cell_type":"code","source":["# Configure dataset\n","\n","class CustomDataset(Dataset):\n","\n","    def __init__(self, data, problem_nums): #initializes the dataset with the folder path, delimiter, and chunk size\n","        # problem_nums_strings = [str(num).zfill(5) for num in problem_nums]\n","        self.data = []\n","\n","        for problem_num in problem_nums:\n","            problem_metadata = \"\\\"\\\"\\\"\\n\" + data[problem_num]['prompt'] + \"\\n\\\"\\\"\\\"\\n\"\n","\n","            self.data.append(problem_metadata + data[problem_num]['code'])\n","\n","    def __len__(self):\n","        return len(self.data)\n","\n","\n","    def __getitem__(self, idx):\n","        return self.data[idx]"],"metadata":{"id":"nySPAOi7bGAo","executionInfo":{"status":"ok","timestamp":1702124764607,"user_tz":300,"elapsed":161,"user":{"displayName":"colabpro gatechmscs","userId":"12293304342394863693"}}},"execution_count":19,"outputs":[]},{"cell_type":"code","source":["# Configer hyperparameters for training\n","\n","num_clients = 2\n","max_length = 150\n","batch_size = 8\n","conduct_logging = True\n","epochs = 1000\n","learning_rate = 4e-4\n","t_max = epochs // 10\n","\n","training_args = {\n","    'clients': num_clients,\n","    'MAX_LENGTH': max_length,\n","    'BATCH_SIZE': batch_size,\n","    'conduct_logging': conduct_logging,\n","    'EPOCHS': epochs,\n","    'lr': learning_rate,\n","    't_max': t_max,\n","}"],"metadata":{"id":"Qsy1zrhpa9VM","executionInfo":{"status":"ok","timestamp":1702127519392,"user_tz":300,"elapsed":123,"user":{"displayName":"colabpro gatechmscs","userId":"12293304342394863693"}}},"execution_count":30,"outputs":[]},{"cell_type":"code","source":["# Configure hyperparameters for model\n","\n","model_name = 'Deci/DeciCoder-1b'\n","\n","tokenizer = AutoTokenizer.from_pretrained(model_name)\n","tokenizer.pad_token = tokenizer.eos_token\n","\n","quant_config = BitsAndBytesConfig(\n","    load_in_4bit=True,\n","    bnb_4bit_use_double_quant=True,\n","    bnb_4bit_quant_type=\"nf4\",\n","    bnb_4bit_compute_dtype=torch.bfloat16,\n",")\n","\n","target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"]\n","# target_modules = [\"gate_proj\", \"up_proj\", \"down_proj\"]\n","lora_config = LoraConfig(\n","    r=8,\n","    target_modules=target_modules,\n","    lora_alpha=32,\n","    lora_dropout=0.05,\n","    bias=\"none\",\n","    task_type=\"CAUSAL_LM\",\n",")\n","\n","model_args = {\n","    'model_name': model_name,\n","    'tokenizer': tokenizer,\n","    'datasets_list': None,\n","    'quant_config': quant_config,\n","    'lora_config': lora_config\n","}"],"metadata":{"id":"PRoUQIlgbpnr","executionInfo":{"status":"ok","timestamp":1702127524638,"user_tz":300,"elapsed":542,"user":{"displayName":"colabpro gatechmscs","userId":"12293304342394863693"}}},"execution_count":31,"outputs":[]},{"cell_type":"code","source":["SAVE_PATH = \"./checkpoints/deci_model_mbpp_2\"\n","LOSSES_PATH = \"./logs/losses/deci_model_mbpp_2_loss\"\n","TIMES_PATH = \"./logs/times/deci_model_mbpp_2_elapsed_time\""],"metadata":{"id":"5ghVfSmgPSil","executionInfo":{"status":"ok","timestamp":1702127562228,"user_tz":300,"elapsed":115,"user":{"displayName":"colabpro gatechmscs","userId":"12293304342394863693"}}},"execution_count":33,"outputs":[]},{"cell_type":"code","source":["# !mkdir ./checkpoints\n","# !mkdir ./logs\n","# !mkdir ./logs/losses\n","# !mkdir ./logs/times"],"metadata":{"id":"prtgBSvnqymj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["mbpp_dataset = load_dataset('mbpp', 'sanitized')\n","mbpp_dataset = mbpp_dataset['train']"],"metadata":{"id":"hHwT9ldmbBJg","executionInfo":{"status":"ok","timestamp":1702124790935,"user_tz":300,"elapsed":1071,"user":{"displayName":"colabpro gatechmscs","userId":"12293304342394863693"}}},"execution_count":23,"outputs":[]},{"cell_type":"code","source":["num_problems_per_client = 8\n","\n","# Generate num_problems_per_client * num_clients random idxs within dataset\n","np.random.seed(0)\n","selection = np.random.choice(np.arange(len(mbpp_dataset)), size=num_clients * num_problems_per_client, replace=False).tolist()\n","\n","custom_dataset = []\n","for i in range(num_clients):\n","    custom_dataset.append(CustomDataset(mbpp_dataset, selection[i * num_problems_per_client: (i + 1) * num_problems_per_client]))\n","\n","model_args['datasets_list'] = custom_dataset"],"metadata":{"id":"WdaITq_ebHla","executionInfo":{"status":"ok","timestamp":1702127563822,"user_tz":300,"elapsed":103,"user":{"displayName":"colabpro gatechmscs","userId":"12293304342394863693"}}},"execution_count":34,"outputs":[]},{"cell_type":"code","source":["# import helpers\n","# import importlib\n","# importlib.reload(helpers)\n","\n","from helpers.train_ronit_modifs import generic_training_runner\n","\n","generic_training_runner(\n","    SAVE_PATH,\n","    LOSSES_PATH,\n","    TIMES_PATH,\n","    model_args,\n","    training_args,\n",")"],"metadata":{"id":"XcGXUKDgQKj6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from transformers import (\n","    AutoTokenizer,\n","    AutoModelForCausalLM,\n","    BitsAndBytesConfig,\n",")\n","\n","import torch\n","\n","from peft import LoraConfig, prepare_model_for_kbit_training, get_peft_model\n","import os\n","import pickle\n","\n","import time\n","from tqdm import tqdm\n","\n","from torch.optim.lr_scheduler import CosineAnnealingLR\n","\n","\n","def generic_training_runner(\n","    SAVE_PATH,\n","    LOSSES_PATH,\n","    TIMES_PATH,\n","    model_info,\n","    training_args,\n","):\n","    \"\"\"\n","    Run training procedure across multiple clients and generate client models\n","\n","    :param SAVE_PATH: Directory to save checkpoints to.\n","    :param LOSSES_PATH: Directory to save all losses in.\n","    :param TIMES_PATH: Directory to save how long each client took to train.\n","\n","\n","    \"\"\"\n","    # Ensure the files can be written without program crash.\n","    assert os.path.exists(os.path.dirname(SAVE_PATH))\n","    assert os.path.exists(os.path.dirname(LOSSES_PATH))\n","    assert os.path.exists(os.path.dirname(TIMES_PATH))\n","\n","    # Args\n","    clients, MAX_LENGTH, BATCH_SIZE, conduct_logging, EPOCHS, lr, t_max = (\n","        training_args[\"clients\"],\n","        training_args[\"MAX_LENGTH\"],\n","        training_args[\"BATCH_SIZE\"],\n","        training_args[\"conduct_logging\"],\n","        training_args[\"EPOCHS\"],\n","        training_args[\"lr\"],\n","        training_args[\"t_max\"],\n","    )\n","    (\n","        model_name,\n","        tokenizer,\n","        datasets_list,\n","        quant_config,\n","        lora_config,\n","    ) = (\n","        model_info[\"model_name\"],\n","        model_info[\"tokenizer\"],\n","        model_info[\"datasets_list\"],\n","        model_info[\"quant_config\"],\n","        model_info[\"lora_config\"],\n","    )\n","\n","    # Device setup\n","    torch.manual_seed(42)\n","    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","    print(f\"Device Type: {device}\")\n","\n","    # Obtain data loaders for each client (separate partitions of data)\n","    print(\"Loading Data and Producing DataLoader objects\")\n","    data_loaders_clients = [\n","        torch.utils.data.DataLoader(\n","            d, batch_size=BATCH_SIZE, shuffle=True, drop_last=True\n","        )\n","        for d in datasets_list\n","    ]\n","    print(\"Finished loading data\")\n","\n","    # Loggings of losses setup.\n","    def write_to_file(epoch_number, loss_values, avg_loss, file_path):\n","        # Open the file in binary append mode\n","        with open(file_path + \".txt\", \"ab\") as file:\n","            # Serialize and write the epoch number\n","            pickle.dump(f\"Epoch {epoch_number}\", file)\n","            # Serialize and write the list of loss values\n","            pickle.dump(loss_values, file)\n","            # Average loss for the epoch\n","            pickle.dump(avg_loss, file)\n","\n","    # Simple test to log.\n","    conduct_logging = True\n","    if conduct_logging:\n","        write_to_file(-1, [-1, -2, -3], [-2], LOSSES_PATH)\n","\n","    # Begin training process\n","    # Traditional training loop requires modifications for FL...\n","    # To do this, we perform the following. Assume only 1 epoch is done.\n","    \"\"\"\n","    Create a variable to aggregate gradients\n","\n","    For Each client\n","        Copy the global model\n","        Fetch the client's dataloader\n","        Train across the data.\n","        Log losses carefully\n","        Save model\n","    \"\"\"\n","\n","    def train_loop(client_idx):\n","        global_model = AutoModelForCausalLM.from_pretrained(\n","            model_name, quantization_config=quant_config, device_map={\"\": 0}\n","        )\n","        global_model.gradient_checkpointing_enable()\n","        global_model = prepare_model_for_kbit_training(global_model)\n","        global_model = get_peft_model(global_model, lora_config).to(device)\n","        client_model = global_model\n","        print(\"Loaded global model.\")\n","\n","        # Set up loss and optimization unique to model.\n","        criterion = torch.nn.CrossEntropyLoss()\n","        optimizer = torch.optim.AdamW(client_model.parameters(), lr=lr)\n","        scheduler = CosineAnnealingLR(optimizer, T_max=t_max)\n","\n","        losses = []\n","        # Log a 10K losses\n","        dataloader_client = data_loaders_clients[client_idx]\n","        log_every = max(1, (EPOCHS * len(dataloader_client)) // 10_000)\n","        print(\"Number of batches:\", len(dataloader_client))\n","        client_model.train()\n","        start_time = time.time()\n","        print(\"Starting training\")\n","        for epoch in range(EPOCHS):\n","            total_loss = 0\n","            for c, batch in tqdm(enumerate(dataloader_client)):\n","                # Batch size x Max Seq LEn\n","                sample = tokenizer(\n","                    batch,\n","                    padding=True,\n","                    truncation=True,\n","                    return_tensors=\"pt\",\n","                    max_length=MAX_LENGTH,\n","                )[\"input_ids\"].to(device)\n","                target = sample.detach()[:, 1:]\n","                sample = sample[:, :-1]\n","\n","                # Batch size x Max Seq Len x Vocab Size\n","                optimizer.zero_grad()\n","                prediction = client_model(sample).logits\n","\n","                # Ensure swapping of axes\n","                loss = criterion(prediction.transpose(1, 2), target)\n","                loss.backward()\n","\n","                # Loss logging\n","                total_loss += loss.item()\n","                if c % log_every == 0:\n","                    print(f\"Step: {c}, Loss: {loss.item():.4f}\")\n","                    losses.append(loss.item())\n","\n","                # Change model weights\n","                optimizer.step()\n","\n","                # Explicit destruction (may not be needed after previous debugging)\n","                del loss, prediction, sample, target\n","                torch.cuda.empty_cache()\n","            scheduler.step()\n","            print(f\"Epoch {epoch} Complete\")\n","            avg_loss = total_loss / len(dataloader_client)\n","            if conduct_logging:\n","                write_to_file(epoch, losses, avg_loss, LOSSES_PATH + f\"_{client_idx}\")\n","            losses.clear()\n","\n","        print(\"Ending training\")\n","\n","        end_time = time.time()\n","\n","        optimizer.zero_grad()\n","\n","        elapsed_time = end_time - start_time\n","        if conduct_logging:\n","            with open(TIMES_PATH + f\"_{client_idx}.txt\", \"a\") as file:\n","                file.write(f\"{elapsed_time}\")\n","        print(elapsed_time)\n","\n","        if conduct_logging:\n","            client_model.save_pretrained(SAVE_PATH + f\"_{client_idx}\")\n","            return client_model\n","            print(\"MODEL SAVED\")\n","            del client_model\n","\n","        torch.cuda.empty_cache()\n","\n","    # Execute training for all clients\n","    client_rets = []\n","    for i in range(clients):\n","        print(f\"Beginning training iteration for client {i}\")\n","        client_rets.append(train_loop(i))\n","    return client_rets\n","\n","client_rets = generic_training_runner(\n","    SAVE_PATH,\n","    LOSSES_PATH,\n","    TIMES_PATH,\n","    model_args,\n","    training_args,\n",")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nGPFTAT9jJuI","executionInfo":{"status":"ok","timestamp":1702127702031,"user_tz":300,"elapsed":120751,"user":{"displayName":"colabpro gatechmscs","userId":"12293304342394863693"}},"outputId":"00c9eca8-94d9-4df5-9ed4-4c522dc8e5b6"},"execution_count":37,"outputs":[{"output_type":"stream","name":"stdout","text":["Device Type: cuda\n","Loading Data and Producing DataLoader objects\n","Finished loading data\n","Beginning training iteration for client 0\n","The repository for Deci/DeciCoder-1b contains custom code which must be executed to correctly load the model. You can inspect the repository content at https://hf.co/Deci/DeciCoder-1b.\n","You can avoid this prompt in future by passing the argument `trust_remote_code=True`.\n","\n","Do you wish to run the custom code? [y/N] y\n","Loaded global model.\n","Number of batches: 1\n","Starting training\n"]},{"output_type":"stream","name":"stderr","text":["1it [00:02,  2.57s/it]\n"]},{"output_type":"stream","name":"stdout","text":["Step: 0, Loss: 5.4841\n","Epoch 0 Complete\n"]},{"output_type":"stream","name":"stderr","text":["1it [00:02,  2.54s/it]\n"]},{"output_type":"stream","name":"stdout","text":["Step: 0, Loss: 4.8760\n","Epoch 1 Complete\n"]},{"output_type":"stream","name":"stderr","text":["1it [00:02,  2.55s/it]\n"]},{"output_type":"stream","name":"stdout","text":["Step: 0, Loss: 4.4399\n","Epoch 2 Complete\n"]},{"output_type":"stream","name":"stderr","text":["1it [00:02,  2.57s/it]\n"]},{"output_type":"stream","name":"stdout","text":["Step: 0, Loss: 4.4383\n","Epoch 3 Complete\n"]},{"output_type":"stream","name":"stderr","text":["1it [00:02,  2.57s/it]\n"]},{"output_type":"stream","name":"stdout","text":["Step: 0, Loss: 3.8141\n","Epoch 4 Complete\n"]},{"output_type":"stream","name":"stderr","text":["1it [00:02,  2.59s/it]\n"]},{"output_type":"stream","name":"stdout","text":["Step: 0, Loss: 2.4834\n","Epoch 5 Complete\n"]},{"output_type":"stream","name":"stderr","text":["1it [00:02,  2.58s/it]\n"]},{"output_type":"stream","name":"stdout","text":["Step: 0, Loss: 1.4011\n","Epoch 6 Complete\n"]},{"output_type":"stream","name":"stderr","text":["1it [00:02,  2.61s/it]\n"]},{"output_type":"stream","name":"stdout","text":["Step: 0, Loss: 1.3996\n","Epoch 7 Complete\n"]},{"output_type":"stream","name":"stderr","text":["1it [00:02,  2.61s/it]\n"]},{"output_type":"stream","name":"stdout","text":["Step: 0, Loss: 0.5334\n","Epoch 8 Complete\n"]},{"output_type":"stream","name":"stderr","text":["1it [00:02,  2.63s/it]\n"]},{"output_type":"stream","name":"stdout","text":["Step: 0, Loss: 0.4714\n","Epoch 9 Complete\n"]},{"output_type":"stream","name":"stderr","text":["1it [00:02,  2.64s/it]\n"]},{"output_type":"stream","name":"stdout","text":["Step: 0, Loss: 0.4616\n","Epoch 10 Complete\n"]},{"output_type":"stream","name":"stderr","text":["1it [00:02,  2.67s/it]\n"]},{"output_type":"stream","name":"stdout","text":["Step: 0, Loss: 0.4632\n","Epoch 11 Complete\n"]},{"output_type":"stream","name":"stderr","text":["1it [00:02,  2.67s/it]\n"]},{"output_type":"stream","name":"stdout","text":["Step: 0, Loss: 0.4292\n","Epoch 12 Complete\n"]},{"output_type":"stream","name":"stderr","text":["1it [00:02,  2.68s/it]\n"]},{"output_type":"stream","name":"stdout","text":["Step: 0, Loss: 0.3519\n","Epoch 13 Complete\n"]},{"output_type":"stream","name":"stderr","text":["1it [00:02,  2.67s/it]\n"]},{"output_type":"stream","name":"stdout","text":["Step: 0, Loss: 0.3157\n","Epoch 14 Complete\n"]},{"output_type":"stream","name":"stderr","text":["1it [00:02,  2.67s/it]\n"]},{"output_type":"stream","name":"stdout","text":["Step: 0, Loss: 0.3162\n","Epoch 15 Complete\n"]},{"output_type":"stream","name":"stderr","text":["1it [00:02,  2.66s/it]\n"]},{"output_type":"stream","name":"stdout","text":["Step: 0, Loss: 0.2714\n","Epoch 16 Complete\n"]},{"output_type":"stream","name":"stderr","text":["1it [00:02,  2.66s/it]\n"]},{"output_type":"stream","name":"stdout","text":["Step: 0, Loss: 0.1868\n","Epoch 17 Complete\n"]},{"output_type":"stream","name":"stderr","text":["1it [00:02,  2.64s/it]\n"]},{"output_type":"stream","name":"stdout","text":["Step: 0, Loss: 0.1515\n","Epoch 18 Complete\n"]},{"output_type":"stream","name":"stderr","text":["1it [00:02,  2.63s/it]"]},{"output_type":"stream","name":"stdout","text":["Step: 0, Loss: 0.1512\n","Epoch 19 Complete\n","Ending training\n","52.565547943115234\n"]},{"output_type":"stream","name":"stderr","text":["\n"]},{"output_type":"stream","name":"stdout","text":["Beginning training iteration for client 1\n","The repository for Deci/DeciCoder-1b contains custom code which must be executed to correctly load the model. You can inspect the repository content at https://hf.co/Deci/DeciCoder-1b.\n","You can avoid this prompt in future by passing the argument `trust_remote_code=True`.\n","\n","Do you wish to run the custom code? [y/N] y\n","Loaded global model.\n","Number of batches: 1\n","Starting training\n"]},{"output_type":"stream","name":"stderr","text":["1it [00:02,  2.38s/it]\n"]},{"output_type":"stream","name":"stdout","text":["Step: 0, Loss: 6.4635\n","Epoch 0 Complete\n"]},{"output_type":"stream","name":"stderr","text":["1it [00:02,  2.31s/it]\n"]},{"output_type":"stream","name":"stdout","text":["Step: 0, Loss: 5.7623\n","Epoch 1 Complete\n"]},{"output_type":"stream","name":"stderr","text":["1it [00:02,  2.32s/it]\n"]},{"output_type":"stream","name":"stdout","text":["Step: 0, Loss: 5.2423\n","Epoch 2 Complete\n"]},{"output_type":"stream","name":"stderr","text":["1it [00:02,  2.32s/it]\n"]},{"output_type":"stream","name":"stdout","text":["Step: 0, Loss: 5.2428\n","Epoch 3 Complete\n"]},{"output_type":"stream","name":"stderr","text":["1it [00:02,  2.33s/it]\n"]},{"output_type":"stream","name":"stdout","text":["Step: 0, Loss: 4.4816\n","Epoch 4 Complete\n"]},{"output_type":"stream","name":"stderr","text":["1it [00:02,  2.34s/it]\n"]},{"output_type":"stream","name":"stdout","text":["Step: 0, Loss: 2.6607\n","Epoch 5 Complete\n"]},{"output_type":"stream","name":"stderr","text":["1it [00:02,  2.34s/it]\n"]},{"output_type":"stream","name":"stdout","text":["Step: 0, Loss: 1.6371\n","Epoch 6 Complete\n"]},{"output_type":"stream","name":"stderr","text":["1it [00:02,  2.36s/it]\n"]},{"output_type":"stream","name":"stdout","text":["Step: 0, Loss: 1.6383\n","Epoch 7 Complete\n"]},{"output_type":"stream","name":"stderr","text":["1it [00:02,  2.36s/it]\n"]},{"output_type":"stream","name":"stdout","text":["Step: 0, Loss: 0.5758\n","Epoch 8 Complete\n"]},{"output_type":"stream","name":"stderr","text":["1it [00:02,  2.37s/it]\n"]},{"output_type":"stream","name":"stdout","text":["Step: 0, Loss: 0.5148\n","Epoch 9 Complete\n"]},{"output_type":"stream","name":"stderr","text":["1it [00:02,  2.38s/it]\n"]},{"output_type":"stream","name":"stdout","text":["Step: 0, Loss: 0.5011\n","Epoch 10 Complete\n"]},{"output_type":"stream","name":"stderr","text":["1it [00:02,  2.38s/it]\n"]},{"output_type":"stream","name":"stdout","text":["Step: 0, Loss: 0.5011\n","Epoch 11 Complete\n"]},{"output_type":"stream","name":"stderr","text":["1it [00:02,  2.38s/it]\n"]},{"output_type":"stream","name":"stdout","text":["Step: 0, Loss: 0.4758\n","Epoch 12 Complete\n"]},{"output_type":"stream","name":"stderr","text":["1it [00:02,  2.39s/it]\n"]},{"output_type":"stream","name":"stdout","text":["Step: 0, Loss: 0.4189\n","Epoch 13 Complete\n"]},{"output_type":"stream","name":"stderr","text":["1it [00:02,  2.40s/it]\n"]},{"output_type":"stream","name":"stdout","text":["Step: 0, Loss: 0.3897\n","Epoch 14 Complete\n"]},{"output_type":"stream","name":"stderr","text":["1it [00:02,  2.40s/it]\n"]},{"output_type":"stream","name":"stdout","text":["Step: 0, Loss: 0.3896\n","Epoch 15 Complete\n"]},{"output_type":"stream","name":"stderr","text":["1it [00:02,  2.40s/it]\n"]},{"output_type":"stream","name":"stdout","text":["Step: 0, Loss: 0.3502\n","Epoch 16 Complete\n"]},{"output_type":"stream","name":"stderr","text":["1it [00:02,  2.40s/it]\n"]},{"output_type":"stream","name":"stdout","text":["Step: 0, Loss: 0.2751\n","Epoch 17 Complete\n"]},{"output_type":"stream","name":"stderr","text":["1it [00:02,  2.39s/it]\n"]},{"output_type":"stream","name":"stdout","text":["Step: 0, Loss: 0.2423\n","Epoch 18 Complete\n"]},{"output_type":"stream","name":"stderr","text":["1it [00:02,  2.39s/it]"]},{"output_type":"stream","name":"stdout","text":["Step: 0, Loss: 0.2413\n","Epoch 19 Complete\n","Ending training\n","47.48504042625427\n"]},{"output_type":"stream","name":"stderr","text":["\n"]}]},{"cell_type":"code","source":["!zip -r /content/trained_decicoder_mbpp.zip content/sumanth_setup"],"metadata":{"id":"S6-DBzbd6mpf"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Upload only zipped sumanth_setup folder"],"metadata":{"id":"jCJYn9NS-qZb"}},{"cell_type":"code","source":["# If directory already exists, skip this\n","!unzip /content/sumanth_setup.zip -d /content/sumanth_setup"],"metadata":{"id":"W7KADgCG7zEN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"E9aexz58B3_M","executionInfo":{"status":"ok","timestamp":1701411562178,"user_tz":300,"elapsed":1568,"user":{"displayName":"ling liu","userId":"12293304342394863693"}},"outputId":"554da735-8f10-4d76-c5b8-c94b0ce38013"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"code","source":["import os\n","os.chdir(\"/content/drive/MyDrive/CS6220 Folder/sumanth_setup\")"],"metadata":{"id":"Edk1vDaYCCon","executionInfo":{"status":"ok","timestamp":1702118536830,"user_tz":300,"elapsed":176,"user":{"displayName":"colabpro gatechmscs","userId":"12293304342394863693"}}},"execution_count":13,"outputs":[]},{"cell_type":"code","source":["import helpers\n","import importlib\n","importlib.reload(helpers)\n","\n","from helpers.evaluation import generate_batch_completions\n","\n","test_prompt = mbpp_dataset[0]['prompt']\n","\n","client_model = AutoModelForCausalLM.from_pretrained(\n","    SAVE_PATH + f\"_{0}\", quantization_config=quant_config, device_map={\"\": 0}\n",")\n","client_model.gradient_checkpointing_enable()\n","\n","client_model = prepare_model_for_kbit_training(client_model)\n","client_model = get_peft_model(client_model, lora_config).to(device)\n"],"metadata":{"id":"Ho7zojWU9DnB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(mbpp_dataset[selection[4]]['prompt'])\n","\n","formatted_prompt = \"\\\"\\\"\\\"\\n\" + mbpp_dataset[selection[4]]['prompt'] + \"\\n\\\"\\\"\\\"\\n\"\n","\n","output = generate_batch_completions([formatted_prompt], client_model, tokenizer)\n","print(output[0])\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6Oe7AobfDwml","executionInfo":{"status":"ok","timestamp":1701413243888,"user_tz":300,"elapsed":16486,"user":{"displayName":"ling liu","userId":"12293304342394863693"}},"outputId":"0b6ef4c2-4c46-42c6-a13c-2120eec148d7"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Write a function to convert tuple string to integer tuple.\n","Device Type: cuda\n","\"\"\"\n","Write a function to convert tuple string to integer tuple.\n","\"\"\"\n","\n","def tuple_to_int(tuple_str):\n","    \"\"\"\n","    Convert tuple string to integer tuple.\n","    \"\"\"\n","    return tuple(int(x) for x in tuple_str)\n","\n","\n"]}]},{"cell_type":"code","source":["global_model = AutoModelForCausalLM.from_pretrained(\n","    SAVE_PATH + f\"_{0}\", quantization_config=quant_config, device_map={\"\": 0}\n",")\n","global_model.gradient_checkpointing_enable()\n","\n","global_model = prepare_model_for_kbit_training(global_model)\n","global_model = get_peft_model(global_model, lora_config).to(device)\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mxf6iR3hGF0T","executionInfo":{"status":"ok","timestamp":1701412690645,"user_tz":300,"elapsed":12637,"user":{"displayName":"ling liu","userId":"12293304342394863693"}},"outputId":"aba30bff-2bb6-4e7d-8e67-fabb1290a769"},"execution_count":null,"outputs":[{"name":"stdout","output_type":"stream","text":["The repository for Deci/DeciCoder-1b contains custom code which must be executed to correctly load the model. You can inspect the repository content at https://hf.co/Deci/DeciCoder-1b.\n","You can avoid this prompt in future by passing the argument `trust_remote_code=True`.\n","\n","Do you wish to run the custom code? [y/N] y\n"]}]},{"cell_type":"code","source":["print(mbpp_dataset[selection[4]]['prompt'])\n","\n","formatted_prompt = \"\\\"\\\"\\\"\\n\" + mbpp_dataset[selection[4]]['prompt'] + \"\\n\\\"\\\"\\\"\\n\"\n","\n","output = generate_batch_completions([formatted_prompt], global_model, tokenizer)\n","print(output[0])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3HO4EFkvGRLp","executionInfo":{"status":"ok","timestamp":1701413253556,"user_tz":300,"elapsed":9690,"user":{"displayName":"ling liu","userId":"12293304342394863693"}},"outputId":"d854d4b6-e2e1-40d6-ffb2-0ef73c38a717"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Write a function to convert tuple string to integer tuple.\n","Device Type: cuda\n","\"\"\"\n","Write a function to convert tuple string to integer tuple.\n","\"\"\"\n","\n","def tuple_to_int(tuple_str):\n","    \"\"\"\n","    Convert tuple string to integer tuple.\n","    \"\"\"\n","    return tuple(int(x) for x in tuple_str)\n","\n","\n"]}]},{"cell_type":"code","source":["# Dealing with above bug in peft library\n","weight_dict = {}\n","for client_idx, client in enumerate(client_rets):\n","    for k,v in client.state_dict().items():\n","        if isinstance(v, str):\n","                weight_dict[k] = v\n","        else:\n","            if client_idx == 0:       # For the first client, we need to initialize the value.\n","                weight_dict[k] = 0\n","            weight_dict[k] += v/torch.tensor(2)\n","aggregated_model = AutoModelForCausalLM.from_pretrained(\n","    model_args['model_name'], quantization_config=model_args['quant_config'], device_map={\"\": 0}\n",")\n","aggregated_model = prepare_model_for_kbit_training(aggregated_model)\n","global_aggregated_model = get_peft_model(aggregated_model, model_args['lora_config']).to('cuda')\n","\n","global_aggregated_model.load_state_dict(weight_dict)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JeHkxmqoMBFx","executionInfo":{"status":"ok","timestamp":1702127813701,"user_tz":300,"elapsed":111677,"user":{"displayName":"colabpro gatechmscs","userId":"12293304342394863693"}},"outputId":"73f24f91-6318-42c0-997e-0c8cc8133bdc"},"execution_count":38,"outputs":[{"name":"stdout","output_type":"stream","text":["The repository for Deci/DeciCoder-1b contains custom code which must be executed to correctly load the model. You can inspect the repository content at https://hf.co/Deci/DeciCoder-1b.\n","You can avoid this prompt in future by passing the argument `trust_remote_code=True`.\n","\n","Do you wish to run the custom code? [y/N] y\n"]},{"output_type":"execute_result","data":{"text/plain":["<All keys matched successfully>"]},"metadata":{},"execution_count":38}]},{"cell_type":"code","source":["import importlib\n","\n","import helpers\n","import helpers.evaluation\n","importlib.reload(helpers)\n","# importlib.reload(helpers.evaluation)\n","\n","from helpers.evaluation import (\n","    generate_batch_completions,\n","    get_humaneval_dataloader,\n","    generate_model_on_problems,\n","    get_prompt_from_descr,\n","    clean_samples,\n","    compute_average_rouge_scores,\n","    print_gens_on_problem_all_models,\n",")\n","\n","import human_eval\n","importlib.reload(human_eval)\n","from human_eval.data import write_jsonl, read_problems\n","\n","problems = read_problems()\n","probs_dataloader = get_humaneval_dataloader(problems, batch_size=10)"],"metadata":{"id":"AAzNujIprnrH","executionInfo":{"status":"ok","timestamp":1702127858715,"user_tz":300,"elapsed":1266,"user":{"displayName":"colabpro gatechmscs","userId":"12293304342394863693"}}},"execution_count":46,"outputs":[]},{"cell_type":"code","source":["samples = generate_model_on_problems(client_rets[0], probs_dataloader, tokenizer)\n","\n","write_jsonl(\"human_eval/samples-client0.json\", samples)\n","\n","!python3 human_eval/evaluate_functional_correctness.py human_eval/samples-client0.json\n","\n","references = [problems[f'HumanEval/{i}']['prompt'] for i in range(len(samples))]\n","hypotheses = [samples[i]['completion'] for i in range(len(samples))]\n","\n","# Compute average ROUGE scores\n","average_rouge_scores = compute_average_rouge_scores(references, hypotheses)\n","print(average_rouge_scores)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jtulHdzfrwGU","executionInfo":{"status":"ok","timestamp":1702129882116,"user_tz":300,"elapsed":2019736,"user":{"displayName":"colabpro gatechmscs","userId":"12293304342394863693"}},"outputId":"3d8f0524-a415-4d6d-be01-be8095d9475a"},"execution_count":47,"outputs":[{"output_type":"stream","name":"stderr","text":["A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"]},{"output_type":"stream","name":"stdout","text":["Device Type: cuda\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n","  warnings.warn(\n","A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"]},{"output_type":"stream","name":"stdout","text":["Generated completions for batch 0 of 17\n","Device Type: cuda\n"]},{"output_type":"stream","name":"stderr","text":["A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"]},{"output_type":"stream","name":"stdout","text":["Generated completions for batch 1 of 17\n","Device Type: cuda\n"]},{"output_type":"stream","name":"stderr","text":["A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"]},{"output_type":"stream","name":"stdout","text":["Generated completions for batch 2 of 17\n","Device Type: cuda\n"]},{"output_type":"stream","name":"stderr","text":["A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"]},{"output_type":"stream","name":"stdout","text":["Generated completions for batch 3 of 17\n","Device Type: cuda\n"]},{"output_type":"stream","name":"stderr","text":["A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"]},{"output_type":"stream","name":"stdout","text":["Generated completions for batch 4 of 17\n","Device Type: cuda\n"]},{"output_type":"stream","name":"stderr","text":["A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"]},{"output_type":"stream","name":"stdout","text":["Generated completions for batch 5 of 17\n","Device Type: cuda\n"]},{"output_type":"stream","name":"stderr","text":["A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"]},{"output_type":"stream","name":"stdout","text":["Generated completions for batch 6 of 17\n","Device Type: cuda\n"]},{"output_type":"stream","name":"stderr","text":["A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"]},{"output_type":"stream","name":"stdout","text":["Generated completions for batch 7 of 17\n","Device Type: cuda\n"]},{"output_type":"stream","name":"stderr","text":["A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"]},{"output_type":"stream","name":"stdout","text":["Generated completions for batch 8 of 17\n","Device Type: cuda\n"]},{"output_type":"stream","name":"stderr","text":["A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"]},{"output_type":"stream","name":"stdout","text":["Generated completions for batch 9 of 17\n","Device Type: cuda\n"]},{"output_type":"stream","name":"stderr","text":["A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"]},{"output_type":"stream","name":"stdout","text":["Generated completions for batch 10 of 17\n","Device Type: cuda\n"]},{"output_type":"stream","name":"stderr","text":["A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"]},{"output_type":"stream","name":"stdout","text":["Generated completions for batch 11 of 17\n","Device Type: cuda\n"]},{"output_type":"stream","name":"stderr","text":["A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"]},{"output_type":"stream","name":"stdout","text":["Generated completions for batch 12 of 17\n","Device Type: cuda\n"]},{"output_type":"stream","name":"stderr","text":["A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"]},{"output_type":"stream","name":"stdout","text":["Generated completions for batch 13 of 17\n","Device Type: cuda\n"]},{"output_type":"stream","name":"stderr","text":["A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"]},{"output_type":"stream","name":"stdout","text":["Generated completions for batch 14 of 17\n","Device Type: cuda\n"]},{"output_type":"stream","name":"stderr","text":["A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"]},{"output_type":"stream","name":"stdout","text":["Generated completions for batch 15 of 17\n","Device Type: cuda\n","Generated completions for batch 16 of 17\n","Traceback (most recent call last):\n","  File \"/content/drive/.shortcut-targets-by-id/1ZKIbzAJY4RXvlu64yVQt4OckCxyNqRJ6/CS6220 Folder/human_eval/evaluate_functional_correctness.py\", line 1, in <module>\n","    import fire\n","ModuleNotFoundError: No module named 'fire'\n","{'rouge-1': {'f': 0.18573097040217917, 'p': 0.19859797568905607, 'r': 0.1996940977583539}, 'rouge-2': {'f': 0.02652934507752419, 'p': 0.028056742609114706, 'r': 0.02863835953123486}, 'rouge-l': {'f': 0.17188680422884156, 'p': 0.1837317726478886, 'r': 0.1849742964462597}}\n"]}]},{"cell_type":"code","source":["samples = generate_model_on_problems(client_rets[1], probs_dataloader, tokenizer)\n","\n","write_jsonl(\"human_eval/samples-client1.json\", samples)\n","\n","!python3 human_eval/evaluate_functional_correctness.py human_eval/samples-client1.json\n","\n","references = [problems[f'HumanEval/{i}']['prompt'] for i in range(len(samples))]\n","hypotheses = [samples[i]['completion'] for i in range(len(samples))]\n","\n","# Compute average ROUGE scores\n","average_rouge_scores = compute_average_rouge_scores(references, hypotheses)\n","print(average_rouge_scores)"],"metadata":{"id":"2Qtm1qlErzaJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["samples = generate_model_on_problems(global_aggregated_model, probs_dataloader, tokenizer)\n","\n","write_jsonl(\"human_eval/samples-global.json\", samples)\n","\n","!python3 human_eval/evaluate_functional_correctness.py human_eval/samples-global.json\n","\n","references = [problems[f'HumanEval/{i}']['prompt'] for i in range(len(samples))]\n","hypotheses = [samples[i]['completion'] for i in range(len(samples))]\n","\n","# Compute average ROUGE scores\n","average_rouge_scores = compute_average_rouge_scores(references, hypotheses)\n","print(average_rouge_scores)"],"metadata":{"id":"iXk0bGtHr0X_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(mbpp_dataset[selection[4]]['prompt'])\n","\n","formatted_prompt = \"\\\"\\\"\\\"\\n\" + mbpp_dataset[selection[4]]['prompt'] + \"\\n\\\"\\\"\\\"\\n\"\n","\n","print(generate_batch_completions([formatted_prompt], client_rets[0], tokenizer)[0])"],"metadata":{"id":"-VvB0MbRv2Sq"},"execution_count":null,"outputs":[]}]}