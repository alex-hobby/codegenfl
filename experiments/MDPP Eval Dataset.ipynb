{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"V100"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bpvFSwqrFAr_","outputId":"d7a15d0e-f6da-4ef9-c6d8-14688f2b1a01","executionInfo":{"status":"ok","timestamp":1701414460360,"user_tz":300,"elapsed":30195,"user":{"displayName":"ling liu","userId":"12293304342394863693"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["The autoreload extension is already loaded. To reload it, use:\n","  %reload_ext autoreload\n","Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["!rm -rf /content/sumanth_setup\n","\n","%load_ext autoreload\n","%autoreload 2\n","\n","import locale\n","def getpreferredencoding(do_setlocale = True):\n","    return \"UTF-8\"\n","locale.getpreferredencoding = getpreferredencoding\n","\n","!pip -q install datasets\n","!pip -q install transformers\n","!pip -q install peft\n","!pip -q install -U bitsandbytes\n","!pip -q install rouge\n","\n","\n","import numpy as np\n","from datasets import load_dataset\n","\n","from transformers import AutoTokenizer, AutoModelForCausalLM\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","#%cd /content/drive/MyDrive/CS6220 Folder #add direct access to the folder if you get an error in this cell\n","import os\n","os.chdir(\"/content/drive/MyDrive/CS6220 Folder\")"]},{"cell_type":"code","source":["device=\"cuda\"\n","!pip install huggingface-cli\n","\n","!huggingface-cli login"],"metadata":{"id":"nBg6RHOAFMey","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1701414462131,"user_tz":300,"elapsed":1780,"user":{"displayName":"ling liu","userId":"12293304342394863693"}},"outputId":"230ed2ad-dc04-4639-8d54-6376d57daa79"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[31mERROR: Could not find a version that satisfies the requirement huggingface-cli (from versions: none)\u001b[0m\u001b[31m\n","\u001b[0m\u001b[31mERROR: No matching distribution found for huggingface-cli\u001b[0m\u001b[31m\n","\u001b[0mToken will not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\n","Token is valid (permission: read).\n","Your token has been saved to /root/.cache/huggingface/token\n","Login successful\n"]}]},{"cell_type":"code","source":["import numpy as np\n","from datasets import load_dataset, concatenate_datasets\n","\n","from transformers import (\n","    AutoTokenizer,\n","    AutoModelWithLMHead,\n","    AutoModelForCausalLM,\n","    BitsAndBytesConfig,\n","    GenerationConfig,\n",")\n","from nltk.translate.bleu_score import sentence_bleu\n","import os\n","\n","\n","from datasets import load_dataset\n","\n","import torch\n","from torch.utils.data import Dataset\n","\n","from peft import LoraConfig, prepare_model_for_kbit_training, get_peft_model\n","\n","import pickle\n","\n","import time\n","from tqdm import tqdm\n","\n","import copy\n","\n","import random\n","\n","from bs4 import BeautifulSoup\n"],"metadata":{"id":"9VZ9Kk1LFPjX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!mkdir /content/sumanth_setup\n","!cp './helpers/' -r '/content/sumanth_setup'"],"metadata":{"id":"KuEvZmolSpzb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["os.chdir('/content/sumanth_setup')"],"metadata":{"id":"L4hdOPiPSiiD"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Load MBPP Dataset\n"],"metadata":{"id":"2xDx_OSXFgZf"}},{"cell_type":"code","source":["# Configure dataset\n","\n","class CustomDataset(Dataset):\n","\n","    def __init__(self, data, problem_nums): #initializes the dataset with the folder path, delimiter, and chunk size\n","        # problem_nums_strings = [str(num).zfill(5) for num in problem_nums]\n","        self.data = []\n","\n","        for problem_num in problem_nums:\n","            problem_metadata = \"\\\"\\\"\\\"\\n\" + data[problem_num]['prompt'] + \"\\n\\\"\\\"\\\"\\n\"\n","\n","            self.data.append(problem_metadata + data[problem_num]['code'])\n","\n","    def __len__(self):\n","        return len(self.data)\n","\n","\n","    def __getitem__(self, idx):\n","        return self.data[idx]"],"metadata":{"id":"nySPAOi7bGAo"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Configer hyperparameters for training\n","\n","num_clients = 2\n","max_length = 150\n","batch_size = 8\n","conduct_logging = True\n","epochs = 1000\n","learning_rate = 4e-4\n","t_max = epochs // 10\n","\n","training_args = {\n","    'clients': num_clients,\n","    'MAX_LENGTH': max_length,\n","    'BATCH_SIZE': batch_size,\n","    'conduct_logging': conduct_logging,\n","    'EPOCHS': epochs,\n","    'lr': learning_rate,\n","    't_max': t_max,\n","}"],"metadata":{"id":"Qsy1zrhpa9VM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Configure hyperparameters for model\n","\n","model_name = 'Deci/DeciCoder-1b'\n","\n","tokenizer = AutoTokenizer.from_pretrained(model_name)\n","tokenizer.pad_token = tokenizer.eos_token\n","\n","quant_config = BitsAndBytesConfig(\n","    load_in_4bit=True,\n","    bnb_4bit_use_double_quant=True,\n","    bnb_4bit_quant_type=\"nf4\",\n","    bnb_4bit_compute_dtype=torch.bfloat16,\n",")\n","\n","target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"]\n","# target_modules = [\"gate_proj\", \"up_proj\", \"down_proj\"]\n","lora_config = LoraConfig(\n","    r=8,\n","    target_modules=target_modules,\n","    lora_alpha=32,\n","    lora_dropout=0.05,\n","    bias=\"none\",\n","    task_type=\"CAUSAL_LM\",\n",")\n","\n","model_args = {\n","    'model_name': model_name,\n","    'tokenizer': tokenizer,\n","    'datasets_list': None,\n","    'quant_config': quant_config,\n","    'lora_config': lora_config\n","}"],"metadata":{"id":"PRoUQIlgbpnr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["SAVE_PATH = \"./checkpoints/deci_model_mbpp\"\n","LOSSES_PATH = \"./logs/losses/deci_model_mbpp_loss\"\n","TIMES_PATH = \"./logs/times/deci_model_mbpp_elapsed_time\""],"metadata":{"id":"5ghVfSmgPSil"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# !mkdir ./checkpoints\n","# !mkdir ./logs\n","# !mkdir ./logs/losses\n","# !mkdir ./logs/times"],"metadata":{"id":"prtgBSvnqymj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["mbpp_dataset = load_dataset('mbpp', 'sanitized')\n","mbpp_dataset = mbpp_dataset['train']"],"metadata":{"id":"hHwT9ldmbBJg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["num_problems_per_client = 8\n","\n","# Generate num_problems_per_client * num_clients random idxs within dataset\n","np.random.seed(0)\n","selection = np.random.choice(np.arange(len(mbpp_dataset)), size=num_clients * num_problems_per_client, replace=False).tolist()\n","\n","custom_dataset = []\n","for i in range(num_clients):\n","    custom_dataset.append(CustomDataset(mbpp_dataset, selection[i * num_problems_per_client: (i + 1) * num_problems_per_client]))\n","\n","model_args['datasets_list'] = custom_dataset"],"metadata":{"id":"WdaITq_ebHla"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# import helpers\n","# import importlib\n","# importlib.reload(helpers)\n","\n","# from helpers.train_ronit_modifs import generic_training_runner\n","\n","generic_training_runner(\n","    SAVE_PATH,\n","    LOSSES_PATH,\n","    TIMES_PATH,\n","    model_args,\n","    training_args,\n",")"],"metadata":{"id":"XcGXUKDgQKj6","colab":{"base_uri":"https://localhost:8080/","height":688},"executionInfo":{"status":"error","timestamp":1701414585151,"user_tz":300,"elapsed":8142,"user":{"displayName":"ling liu","userId":"12293304342394863693"}},"outputId":"4265b99a-d2d0-47fc-fa2c-a09fd97246f7"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Device Type: cuda\n","Loading Data and Producing DataLoader objects\n","Finished loading data\n","Beginning training iteration for client 0\n","The repository for Deci/DeciCoder-1b contains custom code which must be executed to correctly load the model. You can inspect the repository content at https://hf.co/Deci/DeciCoder-1b.\n","You can avoid this prompt in future by passing the argument `trust_remote_code=True`.\n","\n","Do you wish to run the custom code? [y/N] y\n","Loaded global model.\n","Number of batches: 1\n","Starting training\n"]},{"output_type":"stream","name":"stderr","text":["0it [00:00, ?it/s]"]},{"output_type":"stream","name":"stdout","text":["Step: 0, Loss: 5.4777\n"]},{"output_type":"stream","name":"stderr","text":["\n"]},{"output_type":"error","ename":"OutOfMemoryError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)","\u001b[0;32m<ipython-input-61-3b49757e886f>\u001b[0m in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# from helpers.train_ronit_modifs import generic_training_runner\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m generic_training_runner(\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0mSAVE_PATH\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mLOSSES_PATH\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-58-3d6d28dde0a0>\u001b[0m in \u001b[0;36mgeneric_training_runner\u001b[0;34m(SAVE_PATH, LOSSES_PATH, TIMES_PATH, model_info, training_args)\u001b[0m\n\u001b[1;32m    190\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclients\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    191\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Beginning training iteration for client {i}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 192\u001b[0;31m         \u001b[0mtrain_loop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-58-3d6d28dde0a0>\u001b[0m in \u001b[0;36mtrain_loop\u001b[0;34m(client_idx)\u001b[0m\n\u001b[1;32m    156\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m                 \u001b[0;31m# Change model weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 158\u001b[0;31m                 \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    159\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m                 \u001b[0;31m# Explicit destruction (may not be needed after previous debugging)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     66\u001b[0m                 \u001b[0minstance\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_step_count\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m                 \u001b[0mwrapped\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__get__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minstance\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m             \u001b[0;31m# Note that the returned function here is no longer a bound method,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    371\u001b[0m                             )\n\u001b[1;32m    372\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 373\u001b[0;31m                 \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    374\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_optimizer_step_code\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    375\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36m_use_grad\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     74\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_grad_enabled\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefaults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'differentiable'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dynamo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph_break\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m             \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dynamo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph_break\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    171\u001b[0m             \u001b[0mbeta1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbeta2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"betas\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 173\u001b[0;31m             self._init_group(\n\u001b[0m\u001b[1;32m    174\u001b[0m                 \u001b[0mgroup\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m                 \u001b[0mparams_with_grad\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py\u001b[0m in \u001b[0;36m_init_group\u001b[0;34m(self, group, params_with_grad, grads, amsgrad, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps)\u001b[0m\n\u001b[1;32m    119\u001b[0m                 )\n\u001b[1;32m    120\u001b[0m                 \u001b[0;31m# Exponential moving average of gradient values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 121\u001b[0;31m                 state[\"exp_avg\"] = torch.zeros_like(\n\u001b[0m\u001b[1;32m    122\u001b[0m                     \u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemory_format\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreserve_format\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m                 )\n","\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 46.00 MiB. GPU 0 has a total capacty of 15.77 GiB of which 34.12 MiB is free. Process 274871 has 15.74 GiB memory in use. Of the allocated memory 14.40 GiB is allocated by PyTorch, and 128.83 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"]}]},{"cell_type":"code","source":["!zip -r /content/trained_decicoder_mbpp.zip content/sumanth_setup"],"metadata":{"id":"S6-DBzbd6mpf"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Upload only zipped sumanth_setup folder"],"metadata":{"id":"jCJYn9NS-qZb"}},{"cell_type":"code","source":["# If directory already exists, skip this\n","!unzip /content/sumanth_setup.zip -d /content/sumanth_setup"],"metadata":{"id":"W7KADgCG7zEN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"E9aexz58B3_M","executionInfo":{"status":"ok","timestamp":1701411562178,"user_tz":300,"elapsed":1568,"user":{"displayName":"ling liu","userId":"12293304342394863693"}},"outputId":"554da735-8f10-4d76-c5b8-c94b0ce38013"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"code","source":["import os\n","os.chdir(\"/content/drive/MyDrive/CS6220 Folder\")"],"metadata":{"id":"Edk1vDaYCCon"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import helpers\n","import importlib\n","importlib.reload(helpers)\n","\n","from helpers.evaluation import generate_batch_completions\n","\n","test_prompt = mbpp_dataset[0]['prompt']\n","\n","client_model = AutoModelForCausalLM.from_pretrained(\n","    SAVE_PATH + f\"_{0}\", quantization_config=quant_config, device_map={\"\": 0}\n",")\n","client_model.gradient_checkpointing_enable()\n","\n","client_model = prepare_model_for_kbit_training(client_model)\n","client_model = get_peft_model(client_model, lora_config).to(device)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Ho7zojWU9DnB","executionInfo":{"status":"ok","timestamp":1701411775271,"user_tz":300,"elapsed":19420,"user":{"displayName":"ling liu","userId":"12293304342394863693"}},"outputId":"19154587-3c77-44e3-df43-0eef690cf104"},"execution_count":null,"outputs":[{"name":"stdout","output_type":"stream","text":["The repository for Deci/DeciCoder-1b contains custom code which must be executed to correctly load the model. You can inspect the repository content at https://hf.co/Deci/DeciCoder-1b.\n","You can avoid this prompt in future by passing the argument `trust_remote_code=True`.\n","\n","Do you wish to run the custom code? [y/N] y\n","The repository for Deci/DeciCoder-1b contains custom code which must be executed to correctly load the model. You can inspect the repository content at https://hf.co/Deci/DeciCoder-1b.\n","You can avoid this prompt in future by passing the argument `trust_remote_code=True`.\n","\n","Do you wish to run the custom code? [y/N] y\n"]}]},{"cell_type":"code","source":["print(mbpp_dataset[selection[4]]['prompt'])\n","\n","formatted_prompt = \"\\\"\\\"\\\"\\n\" + mbpp_dataset[selection[4]]['prompt'] + \"\\n\\\"\\\"\\\"\\n\"\n","\n","output = generate_batch_completions([formatted_prompt], client_model, tokenizer)\n","print(output[0])\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6Oe7AobfDwml","executionInfo":{"status":"ok","timestamp":1701413243888,"user_tz":300,"elapsed":16486,"user":{"displayName":"ling liu","userId":"12293304342394863693"}},"outputId":"0b6ef4c2-4c46-42c6-a13c-2120eec148d7"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Write a function to convert tuple string to integer tuple.\n","Device Type: cuda\n","\"\"\"\n","Write a function to convert tuple string to integer tuple.\n","\"\"\"\n","\n","def tuple_to_int(tuple_str):\n","    \"\"\"\n","    Convert tuple string to integer tuple.\n","    \"\"\"\n","    return tuple(int(x) for x in tuple_str)\n","\n","\n"]}]},{"cell_type":"code","source":["global_model = AutoModelForCausalLM.from_pretrained(\n","    SAVE_PATH + f\"_{0}\", quantization_config=quant_config, device_map={\"\": 0}\n",")\n","global_model.gradient_checkpointing_enable()\n","\n","global_model = prepare_model_for_kbit_training(global_model)\n","global_model = get_peft_model(global_model, lora_config).to(device)\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mxf6iR3hGF0T","executionInfo":{"status":"ok","timestamp":1701412690645,"user_tz":300,"elapsed":12637,"user":{"displayName":"ling liu","userId":"12293304342394863693"}},"outputId":"aba30bff-2bb6-4e7d-8e67-fabb1290a769"},"execution_count":null,"outputs":[{"name":"stdout","output_type":"stream","text":["The repository for Deci/DeciCoder-1b contains custom code which must be executed to correctly load the model. You can inspect the repository content at https://hf.co/Deci/DeciCoder-1b.\n","You can avoid this prompt in future by passing the argument `trust_remote_code=True`.\n","\n","Do you wish to run the custom code? [y/N] y\n"]}]},{"cell_type":"code","source":["print(mbpp_dataset[selection[4]]['prompt'])\n","\n","formatted_prompt = \"\\\"\\\"\\\"\\n\" + mbpp_dataset[selection[4]]['prompt'] + \"\\n\\\"\\\"\\\"\\n\"\n","\n","output = generate_batch_completions([formatted_prompt], global_model, tokenizer)\n","print(output[0])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3HO4EFkvGRLp","executionInfo":{"status":"ok","timestamp":1701413253556,"user_tz":300,"elapsed":9690,"user":{"displayName":"ling liu","userId":"12293304342394863693"}},"outputId":"d854d4b6-e2e1-40d6-ffb2-0ef73c38a717"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Write a function to convert tuple string to integer tuple.\n","Device Type: cuda\n","\"\"\"\n","Write a function to convert tuple string to integer tuple.\n","\"\"\"\n","\n","def tuple_to_int(tuple_str):\n","    \"\"\"\n","    Convert tuple string to integer tuple.\n","    \"\"\"\n","    return tuple(int(x) for x in tuple_str)\n","\n","\n"]}]}]}